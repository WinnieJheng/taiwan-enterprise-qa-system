{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11340972,"sourceType":"datasetVersion","datasetId":7085907},{"sourceId":11340978,"sourceType":"datasetVersion","datasetId":7085846},{"sourceId":11340987,"sourceType":"datasetVersion","datasetId":7085915}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ¤–ğŸ’¼ Capstone Project: Taiwan Enterprise QA Systemï¼ˆå°ç£ä¼æ¥­å…§éƒ¨æ™ºèƒ½å•ç­”ç³»çµ±ï¼‰\n## ğŸ”Project Introductionï¼ˆå°ˆæ¡ˆç°¡ä»‹ï¼‰\n\n### ğŸ‘‰ Blogpost: https://hackmd.io/zmmxNnCrQMWTchRSEE63Mg\n### ğŸ‘‰ YouTube video: https://youtu.be/66rlfSux6OU\n\nThis project is the capstone submission for the Gen AI Intensive Course (Q1 2025), co-hosted by Google and Kaggle. The core of this project is a prototype of a Generative AI-powered internal Q&A assistant, specifically designed for a Taiwan-based company. The system aims to assist employees in efficiently accessing information related to HR, finance, and IT policies, thereby reducing the repetitive workload on administrative staff.\n\nBuilt on the Retrieval-Augmented Generation (RAG) architecture, this solution integrates the Google Gemini API with ChromaDB, enabling the assistant to retrieve the most relevant content from internal documents and generate accurate, semantically coherent responses in Traditional Chinese. This project demonstrates the practical potential and creative applications of LLMs in internal business environments in Taiwan.\n\n---\n\næœ¬å°ˆæ¡ˆç‚ºåƒåŠ  2025 å¹´ç¬¬ä¸€å­£ Google x Kaggleã€ŠGen AI Intensiveã€‹èª²ç¨‹ä¹‹çµæ¥­å°ˆé¡Œã€‚å°ˆæ¡ˆæ ¸å¿ƒç‚ºä¸€å€‹ä»¥ç”Ÿæˆå¼ AI æŠ€è¡“ç‚ºåŸºç¤ï¼Œå°ˆç‚ºå°ç£å…¬å¸è¨­è¨ˆçš„ã€Œå…§éƒ¨å•ç­”æ©Ÿå™¨äººåŸå‹ã€ï¼Œæ—¨åœ¨å”åŠ©ä¼æ¥­å…§éƒ¨å“¡å·¥æ›´æœ‰æ•ˆç‡åœ°æŸ¥è©¢äººè³‡ã€è²¡å‹™èˆ‡è³‡è¨Šéƒ¨é–€çš„ç›¸é—œåˆ¶åº¦è³‡è¨Šï¼Œæ¸›å°‘é‡è¤‡æ€§è¡Œæ”¿è©¢ç­”æ‰€è€—è²»çš„äººå·¥æˆæœ¬èˆ‡æ™‚é–“ã€‚\n\næœ¬ç³»çµ±æ•´åˆäº† Google Gemini API èˆ‡ ChromaDB å‘é‡è³‡æ–™åº«ï¼Œå¯¦ç¾ Retrieval-Augmented Generationï¼ˆRAGï¼‰æ¶æ§‹ï¼Œèƒ½æ ¹æ“šä½¿ç”¨è€…å•é¡Œï¼Œè‡ªå…§éƒ¨æ–‡ä»¶ä¸­æ“·å–æœ€ç›¸é—œæ®µè½ï¼Œå†æ­é…å¤§èªè¨€æ¨¡å‹ç”Ÿæˆç²¾æº–ã€èªæ„é€£è²«çš„ä¸­æ–‡å›ç­”ã€‚é€éæ­¤å°ˆæ¡ˆï¼Œå±•ç¤ºäº† LLM åœ¨ç¹é«”ä¸­æ–‡ä¼æ¥­å…§éƒ¨æ‡‰ç”¨ä¸Šçš„å¯è¡Œæ€§èˆ‡å‰µæ„æ½›åŠ›ã€‚","metadata":{"_uuid":"149bd171-8ab0-4440-a9f6-41b75d7dbbf3","_cell_guid":"d23c84ad-1eb4-4084-a980-9c236772d801","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## âš™ï¸ Technologies Usedï¼ˆä½¿ç”¨æŠ€è¡“ï¼‰\n\nThis project demonstrates **three official Gen AI capabilities** as defined in the capstone criteria:\n\n1. **Document Understanding**  \nã€€The system loads and parses internal `.docx` files (HR, Finance, IT), segments them into meaningful paragraphs, and prepares them for semantic embedding. This simulates real-world enterprise documents and enables accurate, context-aware retrieval.\n\n2. **Embeddings & Vector Database (Vector Search)**  \nã€€Using the `text-embedding-004` model from the Google Gemini API, each document chunk is embedded into semantic vectors and stored in ChromaDB, a vector database. Queries are also embedded and matched using vector similarity search.\n\n3. **Retrieval-Augmented Generation (RAG)**  \nã€€When a user asks a question, the system retrieves the most relevant document chunks and injects them into a prompt. Gemini then generates grounded answers in Traditional Chinese, ensuring responses are accurate and aligned with company policies.  \nã€€To ensure stable and consistent answers, the model uses a fixed low-temperature setting (`temperature=0`), which reduces hallucination and enhances response reliability in enterprise use cases.\n\nThese three capabilities form a complete RAG-based Q&A system for internal enterprise use. The assistant demonstrates the practical integration of LLMs into real-world workflows and offers a foundation for future expansion.\n\n---\n\næœ¬å°ˆæ¡ˆä¾ç…§å®˜æ–¹å®šç¾©ï¼Œå¯¦ä½œä¸¦å±•ç¤ºäº† **ä¸‰é …æŒ‡å®šçš„ Gen AI èƒ½åŠ›**ï¼š\n\n1. **æ–‡ä»¶ç†è§£ï¼ˆDocument Understandingï¼‰**  \nã€€ç³»çµ±è®€å– `.docx` æ ¼å¼çš„å…¬å¸å…§éƒ¨è¦ç« æ–‡ä»¶ï¼ˆäººè³‡ã€è²¡å‹™ã€è³‡è¨Šï¼‰ï¼Œä¸¦åˆ‡åˆ†ç‚ºæ®µè½èªæ„å–®ä½ï¼Œä½œç‚ºèªæ„å‘é‡åŒ–èˆ‡æª¢ç´¢çš„åŸºç¤ï¼Œæ¨¡æ“¬ä¼æ¥­çœŸå¯¦æ–‡ä»¶æ‡‰ç”¨å ´æ™¯ã€‚\n\n2. **èªæ„å‘é‡èˆ‡å‘é‡è³‡æ–™åº«ï¼ˆEmbeddings & Vector Searchï¼‰**  \nã€€ä½¿ç”¨ Gemini çš„ `text-embedding-004` æ¨¡å‹å°‡æ¯æ®µå…§å®¹è½‰ç‚ºèªæ„å‘é‡ï¼Œä¸¦å„²å­˜åœ¨ ChromaDB è³‡æ–™åº«ä¸­ã€‚ä½¿ç”¨è€…æå•å¾Œï¼Œç³»çµ±é€éèªæ„ç›¸ä¼¼åº¦é€²è¡Œæ®µè½åŒ¹é…èˆ‡æª¢ç´¢ã€‚\n\n3. **æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰**  \nã€€é‡å°æ¯å€‹å•é¡Œï¼Œç³»çµ±è‡ªè³‡æ–™åº«æ‰¾å‡ºæœ€ç›¸é—œæ®µè½ï¼Œæ’å…¥ prompt ä¸­ï¼Œä¸¦ç”± Gemini æ¨¡å‹ç”¢ç”Ÿçµåˆæ–‡ä»¶çŸ¥è­˜çš„å›è¦†ï¼Œä¿è­‰å›è¦†èªæ„é€£è²«ä¸”ç¬¦åˆå…¬å¸åˆ¶åº¦ã€‚  \nã€€æ­¤å¤–ï¼Œç³»çµ±è¨­å®šå›ºå®šä½æº«åº¦ï¼ˆ`temperature=0`ï¼‰ä»¥å¼·åŒ–è¼¸å‡ºä¸€è‡´æ€§ä¸¦é™ä½å¹»è¦ºé¢¨éšªï¼Œæå‡ä¼æ¥­ç’°å¢ƒä¸‹çš„ä¿¡è³´åº¦èˆ‡å¯æ§æ€§ã€‚\n\né€™ä¸‰é …æŠ€è¡“æ•´åˆå½¢æˆä¸€å€‹å¯å¯¦éš›æ‡‰ç”¨çš„ä¼æ¥­å…§éƒ¨å•ç­”è§£æ±ºæ–¹æ¡ˆï¼Œå±•ç¤ºç”Ÿæˆå¼ AI æŠ€è¡“åœ¨ç¹é«”ä¸­æ–‡å ´åŸŸçš„è½åœ°æ½›åŠ›èˆ‡æ“´å±•æ€§ã€‚","metadata":{"_uuid":"76879500-aef0-4775-934a-1594d2ebb51c","_cell_guid":"4e697777-7973-48ed-a844-c08737a8ceb9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## ğŸ”„ System Workflow & Architectureï¼ˆç³»çµ±æµç¨‹èˆ‡æ¶æ§‹ï¼‰\n\nThis project follows a modular architecture based on the Retrieval-Augmented Generation (RAG) pipeline. The full system flow includes document ingestion, embedding generation, vector indexing, similarity search, and response generation. The detailed steps are as follows:\n\n1. Upload `.docx` files containing internal policy documents (e.g., HR, Finance, IT)\n2. Segment documents into smaller text chunks (typically at paragraph level)\n3. Convert each chunk into a semantic vector using `text-embedding-004` (Gemini embedding model)\n4. Store the vectors in ChromaDB to enable semantic similarity search\n5. User submits a natural language query\n6. The system embeds the query and retrieves the top-N most relevant chunks\n7. Retrieved chunks are inserted into a prompt and passed to `generate_content()` for response generation\n8. A fluent and contextually grounded answer is returned in Traditional Chinese\n\n---\n\næœ¬ç³»çµ±æ¡ç”¨ RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰æ¶æ§‹ï¼Œæµç¨‹æ¨¡çµ„åŒ–ã€å¯æ“´å±•ï¼Œæ¶µè“‹å¾è³‡æ–™å‰è™•ç†åˆ°å›ç­”ç”Ÿæˆçš„å®Œæ•´éˆæ¢ã€‚å…·é«”æ­¥é©Ÿå¦‚ä¸‹ï¼š\n\n1. ä¸Šå‚³åŒ…å«å…¬å¸å…§éƒ¨è¦ç« çš„ `.docx` æ–‡ä»¶ï¼ˆå¦‚äººè³‡ã€è²¡å‹™ã€è³‡è¨Šç­‰ï¼‰\n2. å°‡æ–‡ä»¶åˆ‡åˆ†ç‚ºæ®µè½å–®ä½çš„æ–‡å­—å€å¡Š\n3. ä½¿ç”¨ Gemini çš„ `text-embedding-004` æ¨¡å‹å°‡æ¯æ®µè½‰ç‚ºèªæ„å‘é‡\n4. å°‡å‘é‡å„²å­˜æ–¼ ChromaDB å‘é‡è³‡æ–™åº«ä¸­ï¼Œä¾¿æ–¼å¾ŒçºŒèªæ„æª¢ç´¢\n5. ä½¿ç”¨è€…è¼¸å…¥è‡ªç„¶èªè¨€å•é¡Œä½œç‚ºæŸ¥è©¢\n6. ç³»çµ±å°‡å•é¡Œå‘é‡åŒ–ï¼Œä¸¦å¾è³‡æ–™åº«ä¸­æ‰¾å‡ºæœ€ç›¸é—œçš„ N æ®µæ–‡å­—\n7. å°‡æª¢ç´¢æ®µè½æ’å…¥ prompt ä¸­ï¼Œé€å…¥ Gemini `generate_content()` æ¨¡å‹é€²è¡Œå›ç­”ç”Ÿæˆ\n8. æœ€çµ‚å›å‚³ä¸€å‰‡æµæš¢ã€ä¾æ“šæ–‡ä»¶å…§å®¹ç”Ÿæˆçš„ç¹é«”ä¸­æ–‡å›ç­”","metadata":{"_uuid":"d902421d-dd41-47df-9905-a263e849f33e","_cell_guid":"dbbf49bf-0621-44ec-903a-85094d9b7843","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## ğŸ’¡ Use Cases & Innovation Highlightsï¼ˆæ‡‰ç”¨å ´æ™¯èˆ‡å‰µæ„äº®é»ï¼‰\n\nThis internal Q&A assistant was developed to address a real pain point commonly found in Taiwan-based companies: the repetitive administrative questions faced by HR, IT, and Finance departments. Employees often inquire about topics such as leave policies, reimbursement procedures, account permissions, and software installationsâ€”questions that are typically answered manually by staff over and over again.\n\nThe assistant enables employees to ask natural language questions and receive document-grounded answers instantly, improving response speed and reducing the burden on administrative teams. This boosts operational efficiency while freeing up staff to focus on higher-value tasks.\n\n---\n \næœ¬å•ç­”æ©Ÿå™¨äººå°ˆæ¡ˆèšç„¦æ–¼å°ç£ä¼æ¥­ä¸­å¸¸è¦‹çš„è¡Œæ”¿ç—›é»ï¼šä¾†è‡ªå“¡å·¥çš„å¤§é‡é‡è¤‡æ€§è©¢å•ï¼Œç¶“å¸¸å£“å®äººè³‡ã€è³‡è¨Šèˆ‡è²¡å‹™éƒ¨é–€çš„å·¥ä½œæ•ˆç‡ã€‚ä¾‹å¦‚è«‹å‡åˆ¶åº¦ã€å ±å¸³æµç¨‹ã€ç³»çµ±å¸³è™Ÿæ¬Šé™ã€è»Ÿé«”å®‰è£æ–¹å¼ç­‰å•é¡Œï¼Œçš†éœ€ä»°è³´äººå·¥åè¦†èªªæ˜èˆ‡å›è¦†ã€‚\n\næœ¬ç³»çµ±å¯è®“å“¡å·¥ä»¥è‡ªç„¶èªè¨€æå•ï¼Œä¸¦ç«‹å³å–å¾—ä¾†è‡ªå…¬å¸å…§éƒ¨æ–‡ä»¶çš„æº–ç¢ºå›ç­”ï¼Œä¸åƒ…æå‡æŸ¥è©¢æ•ˆç‡ï¼Œä¹Ÿæœ‰æ•ˆé™ä½è¡Œæ”¿éƒ¨é–€çš„å·¥ä½œè² æ“”ï¼Œä½¿å…¶èƒ½å°ˆæ³¨æ–¼æ›´å…·ç­–ç•¥æ€§çš„ä»»å‹™ä¸Šã€‚","metadata":{"_uuid":"bbc04ac7-3aa7-4abc-bef7-cbaa955f69a3","_cell_guid":"7e637dc6-07d6-40f8-a0b2-7b1759f91519","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## ğŸ”š Conclusion & Future Directionsï¼ˆçµèªèˆ‡æœªä¾†å±•æœ›ï¼‰\n\nThis project demonstrates a practical and scalable application of Generative AI technologies in a real-world enterprise setting. From embedding document knowledge to generating stable, policy-aligned responses in Traditional Chinese, the system successfully integrates Gemini's generative capabilities with ChromaDB's retrieval strength in a full RAG pipeline.\n\nThrough this hands-on experience, I gained valuable insights into prompt design, vector database management, and building reliable Q&A workflows with LLMs. More importantly, I learned how to align AI system design with actual user needs in a business environment.\n\nLooking ahead, this system has strong potential to be deployed within a real organizationâ€”either as a chatbot widget on internal portals or integrated with existing employee helpdesk systems. Future improvements include multi-turn conversations, voice interface, support for additional document formats (e.g., PDFs, Google Docs), and **FAQ shortcut buttons** to improve accessibility and ease of use for employees.\n\n---\n\næœ¬å°ˆæ¡ˆæˆåŠŸå±•ç¾äº†ç”Ÿæˆå¼ AI æŠ€è¡“åœ¨ä¼æ¥­å…§éƒ¨å ´åŸŸçš„å¯¦ç”¨æ€§èˆ‡å¯æ“´å±•æ€§ã€‚é€éå‘é‡åŒ–å…§éƒ¨æ–‡ä»¶å…§å®¹ï¼Œæ­é…ç©©å®šç”Ÿæˆçš„ç¹é«”ä¸­æ–‡å›è¦†ï¼Œç³»çµ±å®Œæ•´å¯¦ç¾äº†ä»¥ Gemini èˆ‡ ChromaDB ç‚ºæ ¸å¿ƒçš„ RAG æ¶æ§‹ï¼Œä¸¦èƒ½å°æ‡‰ä¼æ¥­åˆ¶åº¦å•ç­”éœ€æ±‚ã€‚\n\nåœ¨æœ¬æ¬¡å¯¦ä½œä¸­ï¼Œæˆ‘å¯¦éš›æ“ä½œäº† prompt è¨­è¨ˆã€å‘é‡è³‡æ–™åº«å»ºæ§‹èˆ‡ LLM æ‡‰ç”¨æµç¨‹ï¼Œä¹Ÿå­¸æœƒäº†å¦‚ä½•å¾ä½¿ç”¨è€…è§’åº¦å‡ºç™¼ï¼Œè¨­è¨ˆå‡ºè²¼è¿‘éœ€æ±‚çš„ AI ç³»çµ±ã€‚\n\næ­¤ç³»çµ±å…·å‚™å°å…¥ä¼æ¥­å¯¦å‹™çš„æ½›åŠ›ï¼Œä¾‹å¦‚å¯ä½œç‚ºå…§éƒ¨å…¥å£ç¶²ç«™çš„æ™ºæ…§å®¢æœï¼Œæˆ–ä¸²æ¥æ—¢æœ‰çš„äººåŠ›è³‡æºèˆ‡è³‡è¨Šæœå‹™ç³»çµ±ã€‚åŠŸèƒ½ä¸Šå‰‡å¯é€²ä¸€æ­¥æ“´å……ï¼šå¤šè¼ªå°è©±ã€èªéŸ³æŸ¥è©¢ä»‹é¢ã€æ›´å¤šæ–‡ä»¶æ ¼å¼æ”¯æ´ï¼ˆå¦‚ PDFã€Google æ–‡ä»¶ï¼‰ï¼Œä»¥åŠ**FAQ å¿«é€ŸæŸ¥è©¢æŒ‰éˆ•**ç­‰è¼”åŠ©åŠŸèƒ½ï¼Œæå‡ä½¿ç”¨é«”é©—èˆ‡è³‡è¨Šå¯è¿‘æ€§ã€‚","metadata":{"_uuid":"75d5f3e8-ccb1-4dfc-ac38-7a3fca9cc587","_cell_guid":"cee4ed54-ba27-423a-a40e-27b77c341809","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## âš™ï¸ Setupï¼ˆç’°å¢ƒå®‰è£ï¼‰\n\n### ğŸ“¦ Install dependenciesï¼ˆå®‰è£ä¾è³´å¥—ä»¶ï¼‰\n\nThis notebook uses the following libraries:\n\n- `google-genai==1.7.0`: Gemini API client for text generation  \n- `chromadb==0.6.3`: Vector database for semantic search  \n- `python-docx`: For parsing internal `.docx` documents  \n- `protobuf==3.20.3`, `google-api-core==2.11.1`: Compatible versions to avoid dependency conflicts\n\nTo prevent installation errors or warnings caused by background packages  \n(such as `google-cloud-bigtable`, `automl`, or `pandas-gbq`),  \nwe first uninstall these **unused but pre-installed** packages.  \nThis ensures a smooth setup with **no pip errors or warnings**.\n\n---\n\næœ¬ Notebook ä½¿ç”¨ä»¥ä¸‹æ ¸å¿ƒå¥—ä»¶ï¼š\n\n- `google-genai==1.7.0`ï¼šå‘¼å« Gemini API ç”Ÿæˆå›ç­”  \n- `chromadb==0.6.3`ï¼šç”¨æ–¼èªæ„æœå°‹çš„å‘é‡è³‡æ–™åº«  \n- `python-docx`ï¼šè®€å–å…¬å¸å…§éƒ¨ Word æ–‡ä»¶  \n- `protobuf==3.20.3`, `google-api-core==2.11.1`ï¼šç©©å®šç›¸å®¹ç‰ˆæœ¬ï¼Œé¿å…å¥—ä»¶ä¾è³´è¡çª\n\nç”±æ–¼ Kaggle ç’°å¢ƒä¸­é è¨­å®‰è£äº†ä¸€äº›éæœ¬å°ˆæ¡ˆä½¿ç”¨çš„ Google å¥—ä»¶ï¼ˆå¦‚ `pandas-gbq`, `bigtable`, `automl` ç­‰ï¼‰ï¼Œ  \né€™äº›å¥—ä»¶æœƒå° `protobuf` æˆ– `google-api-core` æœ‰ä¸åŒçš„ç‰ˆæœ¬éœ€æ±‚ï¼Œå°è‡´å®‰è£æ™‚å‡ºç¾éŒ¯èª¤æˆ–è­¦å‘Šã€‚\n\nå› æ­¤æˆ‘å€‘å…ˆç§»é™¤é€™äº›ä¸å¿…è¦çš„é è¨­å¥—ä»¶ï¼Œå†å®‰è£å°ˆæ¡ˆçœŸæ­£éœ€è¦çš„ç›¸å®¹ç‰ˆæœ¬ï¼Œç¢ºä¿å®‰è£éç¨‹**é›¶éŒ¯èª¤ã€é›¶è¡çª**ï¼Œæäº¤æ™‚æ›´å®‰å¿ƒã€‚","metadata":{"_uuid":"8c71e63c-4595-4cc8-9a6d-3bc705d15634","_cell_guid":"d4cdd36b-0b20-48a9-bb56-8f0d48ee09c9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ğŸ§¹ Step 1: ç§»é™¤é è¨­ç’°å¢ƒä¸­å¯èƒ½å¼•ç™¼ä¾è³´è¡çªçš„å¥—ä»¶\n!pip uninstall -qqy jupyterlab kfp protobuf google-api-core tensorflow \\\n                   google-cloud-bigtable google-cloud-automl pandas-gbq\n\n# âœ… Step 2: å®‰è£æœ¬å°ˆæ¡ˆæ‰€éœ€çš„ç›¸å®¹ç‰ˆæœ¬å¥—ä»¶ï¼Œç¢ºä¿ä¸æœƒå‡ºç¾ä»»ä½•å®‰è£éŒ¯èª¤\n!pip install -qU \\\n    google-genai==1.7.0 \\\n    chromadb==0.6.3 \\\n    python-docx \\\n    protobuf==3.20.3 \\\n    google-api-core==2.11.1","metadata":{"_uuid":"fef07bc4-b46c-4f66-8113-b5c845e2c76c","_cell_guid":"2d8f8fbb-c982-4537-8575-e1c310cd9179","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:51:14.267015Z","iopub.execute_input":"2025-04-20T15:51:14.267427Z","iopub.status.idle":"2025-04-20T15:52:50.026830Z","shell.execute_reply.started":"2025-04-20T15:51:14.267396Z","shell.execute_reply":"2025-04-20T15:52:50.024907Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸ“¥ Import Gemini SDKï¼ˆåŒ¯å…¥ Gemini SDK ä¸¦ç¢ºèªç‰ˆæœ¬ï¼‰\n  \nWe import the core modules from the `google.genai` library and display the installed version to ensure correct API usage.\n\nåŒ¯å…¥ Gemini API çš„æ ¸å¿ƒæ¨¡çµ„ï¼Œä¸¦é¡¯ç¤ºç›®å‰å®‰è£ç‰ˆæœ¬ï¼Œä»¥ç¢ºä¿å¾ŒçºŒä½¿ç”¨çš„ API æ­£ç¢ºæ€§ã€‚","metadata":{"_uuid":"61b2c8b8-35e2-42a2-addd-8d8cbfeaa410","_cell_guid":"42df0884-2a3c-46a2-8119-c3b8502a89b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown\n\ngenai.__version__","metadata":{"_uuid":"9ec438bf-f692-42bf-8e71-6444c6473d8f","_cell_guid":"7c3dfb6f-0881-4da7-a73e-3099a608e9a4","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:52:50.029341Z","iopub.execute_input":"2025-04-20T15:52:50.029759Z","iopub.status.idle":"2025-04-20T15:52:51.753238Z","shell.execute_reply.started":"2025-04-20T15:52:50.029722Z","shell.execute_reply":"2025-04-20T15:52:51.752223Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸ” Load Gemini API Keyï¼ˆå¾ Kaggle Secret è¼‰å…¥ Gemini é‡‘é‘°ï¼‰\n  \nTo authenticate access to Gemini API, the API key is retrieved securely from Kaggle Secrets. This avoids exposing the key in plain text.\n \nç‚ºäº†å®‰å…¨åœ°ä½¿ç”¨ Gemini APIï¼Œæœ¬å°ˆæ¡ˆé€é Kaggle Secrets å–å¾— API é‡‘é‘°ï¼Œé¿å…å°‡é‡‘é‘°æ˜æ–‡å¯«å…¥ç¨‹å¼ä¸­ã€‚","metadata":{"_uuid":"87276fa4-faaf-4193-a127-695fcf08c20e","_cell_guid":"4a99f54c-821d-4723-8569-ca87ebea0f16","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"_uuid":"a7d2402f-9bda-4c1f-baec-a9288cf311aa","_cell_guid":"518d6fd9-184d-4be7-b565-3c17e3efbbd5","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:52:51.754979Z","iopub.execute_input":"2025-04-20T15:52:51.755473Z","iopub.status.idle":"2025-04-20T15:52:51.861062Z","shell.execute_reply.started":"2025-04-20T15:52:51.755444Z","shell.execute_reply":"2025-04-20T15:52:51.859996Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ“„ Load Internal Documentsï¼ˆè¼‰å…¥å…¬å¸å…§éƒ¨æ–‡ä»¶ï¼‰\n  \nThis step loads three internal `.docx` files related to HR, Finance, and IT policies using the `python-docx` library. Text is cleaned and combined into a list for later processing.\n \nThe three internal documents (HR, Finance, and IT) used in this project are mock data manually created for testing purposes.  \nThe content was designed to simulate realistic company policy documents while avoiding the use of any sensitive or proprietary information.\n\nä½¿ç”¨ `python-docx` å¥—ä»¶è¼‰å…¥ä¸‰ä»½å…§éƒ¨æ–‡ä»¶ï¼ˆäººè³‡ã€è²¡å‹™ã€è³‡è¨Šï¼‰ï¼Œä¸¦å°‡æ¯ä»½æ–‡ä»¶ä¸­çš„æ®µè½æ•´ç†ç‚ºç´”æ–‡å­—ï¼Œå„²å­˜æ–¼ list ä¾›å¾ŒçºŒè™•ç†ä½¿ç”¨ã€‚\n\næœ¬å°ˆæ¡ˆä¸­ä½¿ç”¨çš„ä¸‰ä»½å…¬å¸å…§éƒ¨æ–‡ä»¶ï¼ˆäººè³‡ã€è²¡å‹™ã€è³‡è¨Šï¼‰çš†ç‚ºç‚ºæ¸¬è©¦ç›®çš„æ‰€è‡ªè£½çš„æ¨¡æ“¬è³‡æ–™ã€‚  \nå…§å®¹è¨­è¨ˆæ¨¡æ“¬çœŸå¯¦å…¬å¸åˆ¶åº¦èªªæ˜ï¼Œä¸¦æœªæ¶‰åŠä»»ä½•æ•æ„Ÿæˆ–çœŸå¯¦å•†æ¥­è³‡è¨Šã€‚","metadata":{"_uuid":"6b72a7a6-7b86-4d27-8623-ae518412df54","_cell_guid":"2f6ed738-2df0-438a-bffc-0312c2564253","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from docx import Document\n\ndef load_docx_text(path):\n    doc = Document(path)\n    return \"\\n\".join([para.text.strip() for para in doc.paragraphs if para.text.strip()])\n\n# è¼‰å…¥ä¸‰ä»½æ–‡ä»¶\nhr_doc = load_docx_text(\"/kaggle/input/company-hr-qa/HR_QA.docx\")\nfinance_doc = load_docx_text(\"/kaggle/input/company-finance-qa/Finance_QA.docx\")\nit_doc = load_docx_text(\"/kaggle/input/company-it-qa/IT_QA.docx\")\n\n# å­˜æˆä¸€å€‹ list\ndocuments = [hr_doc, finance_doc, it_doc]\n\n#æª¢æŸ¥å…§å®¹\nfor i, doc in enumerate(documents):\n    print(f\"Document {i+1} preview:\\n{doc[:300]}\\n{'-'*40}\")","metadata":{"_uuid":"1324c6cb-8e4f-4c5d-b452-5da365ee33ae","_cell_guid":"f4a3617d-97b7-493e-9d0a-9ce2c0bb17c6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:52:51.862908Z","iopub.execute_input":"2025-04-20T15:52:51.863303Z","iopub.status.idle":"2025-04-20T15:52:52.180090Z","shell.execute_reply.started":"2025-04-20T15:52:51.863263Z","shell.execute_reply":"2025-04-20T15:52:52.178835Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ¤– Initialize Gemini Clientï¼ˆåˆå§‹åŒ– Gemini ä¸¦åˆ—å‡ºæ”¯æ´æ¨¡å‹ï¼‰\n\nCreate a Gemini client instance and list available models that support `embedContent`. This ensures we are using a model compatible with the embedding task.\n\nåˆå§‹åŒ– Gemini ç”¨æˆ¶ç«¯ï¼Œä¸¦åˆ—å‡ºæ”¯æ´ `embedContent` åŠŸèƒ½çš„æ¨¡å‹ï¼Œä»¥ç¢ºèªæ‰€é¸æ¨¡å‹å¯åŸ·è¡Œèªæ„å‘é‡ç”Ÿæˆä»»å‹™ã€‚","metadata":{"_uuid":"ca32f42c-8ab0-45be-b736-b901dfcf17e0","_cell_guid":"ce35622b-89ad-4dc6-8c36-750ce656ac8c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nfor m in client.models.list():\n    if \"embedContent\" in m.supported_actions:\n        print(m.name)","metadata":{"_uuid":"64f3de22-c7a1-45e4-8621-747023b4913a","_cell_guid":"4ea350c4-90b7-469f-84f2-6eff0c05d251","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:52:52.181175Z","iopub.execute_input":"2025-04-20T15:52:52.181771Z","iopub.status.idle":"2025-04-20T15:52:52.833642Z","shell.execute_reply.started":"2025-04-20T15:52:52.181729Z","shell.execute_reply":"2025-04-20T15:52:52.832550Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ§  Define GeminiEmbeddingFunctionï¼ˆå®šç¾©åµŒå…¥å‡½å¼ä¾› ChromaDB ä½¿ç”¨ï¼‰\n \nDefine a custom class `GeminiEmbeddingFunction` that uses Geminiâ€™s `text-embedding-004` model to generate embeddings. Includes retry logic for quota-based API errors.\n\nè‡ªè¨‚ä¸€å€‹åµŒå…¥å‡½å¼ `GeminiEmbeddingFunction`ï¼Œä½¿ç”¨ Gemini æ¨¡å‹ `text-embedding-004` é€²è¡Œèªæ„å‘é‡ç”Ÿæˆï¼Œä¸¦åŠ å…¥è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼Œä»¥è™•ç†é…é¡éŒ¯èª¤ã€‚","metadata":{"_uuid":"2978f435-ba72-4fb9-b442-3602facfc9a5","_cell_guid":"e9c57b40-4190-472c-89c9-a62b470d7478","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom google.api_core import retry\n\nfrom google.genai import types\n\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    # Specify whether to generate embeddings for documents, or queries\n    document_mode = True\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -> Embeddings:\n        if self.document_mode:\n            embedding_task = \"retrieval_document\"\n        else:\n            embedding_task = \"retrieval_query\"\n\n        response = client.models.embed_content(\n            model=\"models/text-embedding-004\",\n            contents=input,\n            config=types.EmbedContentConfig(\n                task_type=embedding_task,\n            ),\n        )\n        return [e.values for e in response.embeddings]","metadata":{"_uuid":"cb3d3196-81c5-40e5-8b61-db6495ad336d","_cell_guid":"622d69fa-cade-4a5e-86b2-7beb81a753b6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:52:52.834921Z","iopub.execute_input":"2025-04-20T15:52:52.835458Z","iopub.status.idle":"2025-04-20T15:52:53.772789Z","shell.execute_reply.started":"2025-04-20T15:52:52.835419Z","shell.execute_reply":"2025-04-20T15:52:53.771631Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ—ƒï¸ Initialize Vector Storeï¼ˆåˆå§‹åŒ–å‘é‡è³‡æ–™åº«ï¼‰\n \nSet up a ChromaDB vector database with the custom Gemini embedding function. Add the internal documents to the collection, ready for similarity-based retrieval.\n \nä½¿ç”¨å…ˆå‰å®šç¾©çš„ Gemini åµŒå…¥å‡½å¼åˆå§‹åŒ– ChromaDBï¼Œä¸¦å°‡ä¸‰ä»½å…§éƒ¨æ–‡ä»¶åŠ å…¥è³‡æ–™åº«ï¼Œä»¥åˆ©å¾ŒçºŒèªæ„ç›¸ä¼¼åº¦æŸ¥è©¢ã€‚","metadata":{"_uuid":"ac57e1bd-8c91-4386-8850-111f8257c75f","_cell_guid":"f8dca81a-7ed6-437d-9824-9729c7c3e387","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import chromadb\n\nDB_NAME = \"googlecardb\"\n\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True\n\nchroma_client = chromadb.Client()\ndb = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n\ndb.add(documents=documents, ids=[str(i) for i in range(len(documents))])","metadata":{"_uuid":"608a24b8-b0dc-4355-bbf7-5dc7163107dc","_cell_guid":"981b2432-7ffd-43c9-8676-661f1b613ce9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:52:53.773914Z","iopub.execute_input":"2025-04-20T15:52:53.774410Z","iopub.status.idle":"2025-04-20T15:52:54.377157Z","shell.execute_reply.started":"2025-04-20T15:52:53.774383Z","shell.execute_reply":"2025-04-20T15:52:54.376248Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ğŸ’¬ Generate Answer from User Queryï¼ˆæ ¹æ“šä½¿ç”¨è€…æå•ç”¢ç”Ÿå›è¦†ï¼‰\n\nThis part switches the embedding function into query mode, allowing user questions to be embedded and used for vector search. The system retrieves the top 3 most relevant document chunks from ChromaDB and injects them into a carefully designed prompt.\n\nThe prompt guides Gemini to act as a helpful assistant, replying in clear and friendly Traditional Chinese suitable for non-technical employees. If no relevant document content is found, the model is instructed to answer using general Gemini knowledge.\n\næ­¤éƒ¨åˆ†å°‡åµŒå…¥å‡½å¼åˆ‡æ›ç‚ºã€ŒæŸ¥è©¢æ¨¡å¼ã€ï¼Œä½¿ä½¿ç”¨è€…çš„å•é¡Œä¹Ÿèƒ½é€²è¡Œèªæ„åµŒå…¥èˆ‡æª¢ç´¢ã€‚ç³»çµ±å¾è³‡æ–™åº«ä¸­å–å‡ºæœ€ç›¸é—œçš„ä¸‰æ®µå…§éƒ¨æ–‡ä»¶å…§å®¹ï¼Œä¸¦å°‡å…¶æ•´åˆè‡³ç²¾å¿ƒè¨­è¨ˆçš„æç¤ºè©ï¼ˆpromptï¼‰ä¸­ã€‚\n\næç¤ºå…§å®¹å¼•å° Gemini æ‰®æ¼”ä¸€ä½è¦ªåˆ‡ã€çŸ¥è­˜è±å¯Œçš„åŠ©ç†ï¼Œä»¥è‡ªç„¶æµæš¢çš„ã€å°ç£ç¹é«”ä¸­æ–‡ã€‘å›ç­”å•é¡Œï¼Œé©åˆéæŠ€è¡“èƒŒæ™¯çš„å“¡å·¥é–±è®€ã€‚è‹¥æ–‡ä»¶ç„¡ç›¸é—œè³‡è¨Šï¼Œå‰‡å…è¨± Gemini å›å¾©ä¸€èˆ¬çŸ¥è­˜å‹ç­”æ¡ˆã€‚","metadata":{"_uuid":"c25faa1d-3133-4226-bb71-99d06d7c20a9","_cell_guid":"cfe0deaa-9f4b-4f4d-839c-047bee481ff8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"### ğŸ§ª Test Four Query Scenariosï¼ˆæ¸¬è©¦å››ç¨®æŸ¥è©¢æƒ…å¢ƒï¼‰\n\nTo demonstrate the system's ability to handle different query types, we run four representative questions:\n\n- An HR-related question (document-supported)\n- A finance-related question (document-supported)\n- An IT-related question (document-supported)\n- A general question outside document scope (fallback to Gemini knowledge)\n\nEach question is used to perform vector search, and the retrieved passages are passed into the Gemini prompt to generate the final answer in Traditional Chinese.\n\næœ¬æ®µç¤ºç¯„ç³»çµ±å¦‚ä½•è™•ç†å››ç¨®å¸¸è¦‹å•é¡Œé¡å‹ï¼š\n\n- äººäº‹éƒ¨é–€ç›¸é—œï¼ˆå¯ç”±æ–‡ä»¶å›ç­”ï¼‰\n- è²¡å‹™åˆ¶åº¦ç›¸é—œï¼ˆå¯ç”±æ–‡ä»¶å›ç­”ï¼‰\n- è³‡è¨Šä½œæ¥­ç›¸é—œï¼ˆå¯ç”±æ–‡ä»¶å›ç­”ï¼‰\n- å…¬å¸æ–‡ä»¶ç¯„åœå¤–å•é¡Œï¼ˆç”± Gemini è‡ªæœ‰çŸ¥è­˜ç”Ÿæˆï¼‰\n\næ¯å€‹å•é¡Œçš†ç¶“ç”±èªæ„æª¢ç´¢æ‰¾å‡ºç›¸é—œæ®µè½ï¼Œä¸¦å‚³å…¥ prompt ä¸­è«‹ Gemini ç”¢ç”Ÿæœ€çµ‚å›æ‡‰ã€‚","metadata":{"_uuid":"48bbca99-c00a-48f0-8c0f-0be201eda726","_cell_guid":"314888b5-2507-4ef6-8cc5-76c2516e5ec0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ğŸ§ª Four test queries (ä¸­æ–‡ + è‹±æ–‡è§£é‡‹)\ntest_questions = [\n    (\"å¦‚ä½•è«‹ç—…å‡\", \"HR-related question: How to apply for sick leave\"),\n    (\"å¦‚ä½•å ±å·®æ—…è²»\", \"Finance-related question: How to file travel expense reimbursement\"),\n    (\"VPNç„¡æ³•é€£ç·š\", \"IT-related question: VPN connection issue\"),\n    (\"Excel çš„ SUM å‡½æ•¸æ€éº¼å¯«ï¼Ÿ\", \"Out-of-scope question: How to write the SUM function in Excel?\")\n]\n\n# Set to query mode for question embedding\nembed_fn.document_mode = False\n\n# Iterate over each test query\nfor query_zh, label_en in test_questions:\n    print(f\"\\nğŸ” {label_en}\\nâ“ ä¸­æ–‡å•é¡Œï¼š{query_zh}\")\n    \n    result = db.query(query_texts=[query_zh], n_results=3)\n    [all_passages] = result[\"documents\"]\n    \n    query_oneline = query_zh.replace(\"\\n\", \" \")\n\n    # Prompt includes instruction to return both Chinese and English\n    prompt = f\"\"\"ä½ æ˜¯ä¸€å€‹è¦ªåˆ‡ä¸”çŸ¥è­˜è±å¯Œçš„ AI åŠ©ç†ï¼Œæœƒæ ¹æ“šå…¬å¸å…§éƒ¨æ–‡ä»¶å…§å®¹ä¾†å›ç­”å•é¡Œã€‚\nè«‹ä»¥ã€å°ç£ç¹é«”ä¸­æ–‡ã€‘å›ç­”ï¼Œèªæ°£è¦è‡ªç„¶ã€æ¸…æ¥šï¼Œé©åˆçµ¦éæŠ€è¡“èƒŒæ™¯çš„ä¸€èˆ¬å“¡å·¥é–±è®€ã€‚\nè«‹å‹™å¿…ç”¨å®Œæ•´å¥å­å›ç­”å•é¡Œï¼Œå…§å®¹è¦è©³ç´°ï¼Œè‹¥æœ‰èƒŒæ™¯è³‡æ–™å¯ä»¥ä¸€èµ·è£œå……èªªæ˜ã€‚\nè«‹åœ¨ä¸­æ–‡å›ç­”å¾Œï¼Œ**é™„ä¸Šå°æ‡‰çš„è‹±æ–‡ç¿»è­¯ç‰ˆæœ¬**ã€‚\nå¦‚æœæä¾›çš„æ®µè½è·Ÿå•é¡Œç„¡é—œï¼Œä½ å¯ä»¥å¿½ç•¥é‚£äº›æ®µè½ã€‚\nå¦‚æœä½¿ç”¨è€…å•çš„æ˜¯å…¬å¸å…§éƒ¨æ–‡ä»¶å…§å®¹ä»¥å¤–çš„å•é¡Œï¼Œè«‹ä»¥åŸæœ¬ Gemini çš„æ•¸æ“šç”Ÿæˆå›ç­”ï¼Œä¸¦åŒæ¨£æä¾›ä¸­è‹±æ–‡ç‰ˆæœ¬ã€‚\n\nYou are a helpful and knowledgeable AI assistant. Please answer the following question based on internal company documents.\nRespond in **Traditional Chinese**, using clear and friendly language suitable for non-technical employees.\nPlease use complete sentences and provide detailed answers. If helpful, include relevant background information.\n**After the Traditional Chinese response, please provide an English translation of your answer.**\nIf any retrieved content is irrelevant, you may ignore it.\nIf the question is beyond the scope of internal documents, answer as Gemini normally would, and still provide both Chinese and English versions.\n\nå•é¡Œ (Question)ï¼š{query_oneline}\n\"\"\"\n\n\n    # Append retrieved document passages\n    for passage in all_passages:\n        passage_oneline = passage.replace(\"\\n\", \" \")\n        prompt += f\"æ–‡ä»¶æ®µè½ï¼š{passage_oneline}\\n\"\n\n    # Generate answer using Gemini\n    low_temp_config = types.GenerateContentConfig(temperature=0)\n    answer = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=prompt,\n        config=low_temp_config\n    )\n\n\n    display(Markdown(f\"ğŸ’¡ Gemini å›ç­”ï¼ˆAnswerï¼‰ï¼š\\n\\n{answer.text}\\n\\n{'-'*80}\"))","metadata":{"_uuid":"fd2e5bc0-3d61-4cc2-9897-299f06325528","_cell_guid":"a14ab2bc-564a-4f11-b61a-a7ec601cccc2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:52:54.379372Z","iopub.execute_input":"2025-04-20T15:52:54.379695Z","iopub.status.idle":"2025-04-20T15:53:04.690380Z","shell.execute_reply.started":"2025-04-20T15:52:54.379668Z","shell.execute_reply":"2025-04-20T15:53:04.688727Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ğŸ§ª User Input Demo Sectionï¼ˆä½¿ç”¨è€…äº’å‹•è¼¸å…¥å€å¡Š ï¼‰\n\nIn the previous section, we demonstrated responses to four common office-related questions to showcase the systemâ€™s capability.\nThis section is designed to allow users to input their own question for a more personalized and interactive experience.\n\nâš ï¸ Note: Using Python's input() in a Kaggle Notebook will cause an ERROR during â€œSave Versionâ€ or auto-execution, interrupting the notebook run.\nTo avoid this, we set a default question \"è«‹å•è¦æ€éº¼è«‹ç—…å‡ï¼Ÿ\" in the code as a fallback to ensure successful execution.\n\nğŸ”§ If youâ€™d like to enter your own question, simply uncomment the line below by removing the #:\n```python\n#query = input(\"è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š\")\n```\nå‰é¢æˆ‘å€‘ç¤ºç¯„äº† **å››ç¨®å¸¸è¦‹è¾¦å…¬å®¤å•é¡Œ** çš„æŸ¥è©¢çµæœï¼Œè®“å¤§å®¶å¿«é€Ÿäº†è§£ç³»çµ±çš„åŠŸèƒ½ã€‚  \næœ¬å€å¡Šå‰‡æ˜¯è¨­è¨ˆè®“ä½¿ç”¨è€…å¯ä»¥ **è‡ªè¡Œè¼¸å…¥å•é¡Œ**ï¼Œé«”é©—æ›´å€‹äººåŒ–çš„äº’å‹•å›æ‡‰ã€‚\n\nâš ï¸ **æé†’**ï¼šç”±æ–¼ Kaggle Notebook åœ¨å„²å­˜ï¼ˆSave Versionï¼‰æˆ–åŸ·è¡Œæ•´ä»½ç­†è¨˜æ™‚ï¼Œç„¡æ³•è™•ç† `input()` å‡½å¼ï¼Œæœƒå°è‡´ç¨‹å¼å‡ºç¾ `ERROR` ä¸¦ä¸­æ–·åŸ·è¡Œã€‚  \nå› æ­¤åœ¨ç¨‹å¼ä¸­ï¼Œæˆ‘å€‘é è¨­æ”¾å…¥äº†ä¸€å€‹ç¯„ä¾‹å•é¡Œ `\"è«‹å•è¦æ€éº¼è«‹ç—…å‡ï¼Ÿ\"` ä½œç‚ºå‚™ç”¨ï¼Œç¢ºä¿ Notebook å¯ä»¥é †åˆ©åŸ·è¡Œã€‚\n\nğŸ”§ **å¦‚æœæ‚¨å¸Œæœ›è¼¸å…¥è‡ªå·±çš„å•é¡Œ**ï¼Œè«‹å°‡ä¸‹åˆ—ç¨‹å¼ç¢¼ä¸­çš„è¨»è§£è§£é™¤ï¼ˆåˆªé™¤å‰é¢çš„ `#`ï¼‰å³å¯å•Ÿç”¨äº’å‹•è¼¸å…¥æ¨¡å¼ï¼š\n```python\n#query = input(\"è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š\")\n```","metadata":{"_uuid":"f07e6418-1041-494e-85d0-be3f08b2caf9","_cell_guid":"99277a56-4ccd-4110-9ca3-70ad68392776","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Switch to query mode for embedding user questions (vs. document embedding)\nembed_fn.document_mode = False\n\n# ğŸ’¬ é è¨­æŸ¥è©¢å•é¡Œï¼ˆKaggle notebook ä¸­ä½¿ç”¨ï¼‰\n# For Kaggle notebook auto-execution: preset a default query\n# ğŸ“Œ é€™æ¨£è¨­è¨ˆæ˜¯ç‚ºäº†é¿å…ä½¿ç”¨ input() å°è‡´ notebook ç„¡æ³•è‡ªå‹•åŸ·è¡Œä¸¦å‡ºç¾éŒ¯èª¤\n# ğŸ“Œ This avoids using input() which causes errors during auto-execution in Kaggle notebooks\nquery = \"è«‹å•è¦æ€éº¼è«‹ç—…å‡ï¼Ÿ\"  # Default query for demo purposes\n\n# â— å¦‚æœæ‚¨å¸Œæœ›è‡ªè¡Œè¼¸å…¥å•é¡Œï¼Œè«‹å–æ¶ˆä¸‹æ–¹ input() é€™è¡Œçš„è¨»è§£\n# â— To input your own question, uncomment the line below\n#query = input(\"è«‹è¼¸å…¥æ‚¨çš„å•é¡Œï¼š\")  # Please enter your question (in Traditional Chinese)\n\n\n# Perform semantic search in ChromaDB using the input query\nresult = db.query(query_texts=[query], n_results=3)\n[all_passages] = result[\"documents\"]\n\n# Sanitize query by removing newline characters\nquery_oneline = query.replace(\"\\n\", \" \")\n\n# Build a bilingual prompt for Gemini model\nprompt = f\"\"\"ä½ æ˜¯ä¸€å€‹è¦ªåˆ‡ä¸”çŸ¥è­˜è±å¯Œçš„ AI åŠ©ç†ï¼Œæœƒæ ¹æ“šå…¬å¸å…§éƒ¨æ–‡ä»¶å…§å®¹ä¾†å›ç­”å•é¡Œã€‚\nè«‹ä»¥ã€å°ç£ç¹é«”ä¸­æ–‡ã€‘å›ç­”ï¼Œèªæ°£è¦è‡ªç„¶ã€æ¸…æ¥šï¼Œé©åˆçµ¦éæŠ€è¡“èƒŒæ™¯çš„ä¸€èˆ¬å“¡å·¥é–±è®€ã€‚\nè«‹å‹™å¿…ç”¨å®Œæ•´å¥å­å›ç­”å•é¡Œï¼Œå…§å®¹è¦è©³ç´°ï¼Œè‹¥æœ‰èƒŒæ™¯è³‡æ–™å¯ä»¥ä¸€èµ·è£œå……èªªæ˜ã€‚\nè«‹åœ¨ä¸­æ–‡å›ç­”å¾Œï¼Œ**é™„ä¸Šå°æ‡‰çš„è‹±æ–‡ç¿»è­¯ç‰ˆæœ¬**ã€‚\nå¦‚æœæä¾›çš„æ®µè½è·Ÿå•é¡Œç„¡é—œï¼Œä½ å¯ä»¥å¿½ç•¥é‚£äº›æ®µè½ã€‚\nå¦‚æœä½¿ç”¨è€…å•çš„æ˜¯å…¬å¸å…§éƒ¨æ–‡ä»¶å…§å®¹ä»¥å¤–çš„å•é¡Œï¼Œè«‹ä»¥åŸæœ¬ Gemini çš„æ•¸æ“šç”Ÿæˆå›ç­”ï¼Œä¸¦åŒæ¨£æä¾›ä¸­è‹±æ–‡ç‰ˆæœ¬ã€‚\n\nYou are a helpful and knowledgeable AI assistant. Please answer the following question based on internal company documents.\nRespond in **Traditional Chinese**, using clear and friendly language suitable for non-technical employees.\nPlease use complete sentences and provide detailed answers. If helpful, include relevant background information.\n**After the Traditional Chinese response, please provide an English translation of your answer.**\nIf any retrieved content is irrelevant, you may ignore it.\nIf the question is beyond the scope of internal documents, answer as Gemini normally would, and still provide both Chinese and English versions.\n\nå•é¡Œ (Question)ï¼š{query_oneline}\n\"\"\"\n\n\n# Append retrieved document passages to the prompt\nfor passage in all_passages:\n    passage_oneline = passage.replace(\"\\n\", \" \")\n    prompt += f\"æ–‡ä»¶æ®µè½ (Document passage)ï¼š{passage_oneline}\\n\"\n\n# Send the prompt to Gemini model and generate an answer\nlow_temp_config = types.GenerateContentConfig(temperature=0)\nanswer = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=prompt,\n    config=low_temp_config\n)\n\n# Display the result as Markdown-formatted output\nMarkdown(answer.text)","metadata":{"_uuid":"5c7302bd-65f8-416b-aaa8-d6a0d4465516","_cell_guid":"0ac8917e-cf47-4e21-9b68-c64002491b8a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-20T15:53:04.692933Z","iopub.execute_input":"2025-04-20T15:53:04.693421Z","iopub.status.idle":"2025-04-20T15:53:06.017260Z","shell.execute_reply.started":"2025-04-20T15:53:04.693370Z","shell.execute_reply":"2025-04-20T15:53:06.016187Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}